install.packages(c("dplyr", "ggplot2", "interplot", "margins"))
knitr::opts_chunk$set(echo = TRUE)
?grep
package <- "http://www.difjdss.com"
grep(pattern = "http://", x = package)
package <- "https://www.difjdss.com"
grep(pattern = "http://", x = package)
grep(pattern = "http://", x = package) %>% length
library(dplyr)
library(dplyr)
grep(pattern = "http://", x = package) %>% length
grep(pattern = "https://", x = package) %>% length
?match
"https://" %in% package
grep(pattern = "https://", x = package)
ifelse(length(grep(pattern = "http://", x = package)) +
length(grep(pattern = "https://", x = package) > 0,
TRUE,
FALSE)
ifelse(length(grep(pattern = "http://", x = package)) +
length(grep(pattern = "https://", x = package)) > 0,
TRUE,
FALSE)
length(grep(pattern = "https://", x = package))
length(grep(pattern = "http://", x = package))
?available.packages()
available.packages()
available.packages()[,1]
available.packages() %>% str
a <- available.packages()
View(a)
paste0("http://cran.r-project.org/src/contrib/Archive/",
package,"/",package,"_",version,".tar.gz")
package <- "ggplot2",
paste0("http://cran.r-project.org/src/contrib/Archive/",
package,"/",package,"_",version,".tar.gz")
package <- "ggplot2"
version <- "2.030"
paste0("http://cran.r-project.org/src/contrib/Archive/",
package,"/",package,"_",version,".tar.gz")
installed.packages()[ ,1]
installed.packages()[ ,2]
installed.packages()[ ,3]
installed.packages()["MASS",3]
a <- "rere_erere_erere_erere_re"
gsub('(.*)_\\w+', '\\1', a)
sub("_[^_]+$", "", a)
x <- "https://cran.r-project.org/src/contrib/Archive/ggplot2/ggplot2_0.4.5.tar.gz""
x <- "https://cran.r-project.org/src/contrib/Archive/ggplot2/ggplot2_0.4.5.tar.gz"
sub('.*\\.', '', x)
sub('.*\\.', '', x)
x <- "https://cran.r-project.org/src/contrib/Archive/ggplot2/ggplot2_0.4.5.tar.gz"
sub('.*\\.', '', x)
x <- "https://cran.r-project.org/src/contrib/Archive/ggplot2/ggplot2_0.4.5.tar.gz"
sub('.*\\/', '', x)
package <- "https://cran.r-project.org/src/contrib/Archive/ggplot2/ggplot2_0.4.5.tar.gz"
sub('.*\\/', '', package)
sub('.tar*', '', x)
sub('.tar.*', '', x)
sub('.*\\/.tar.*', '', package)
w <- sub('.*\\/', '', package)
package <- "https://cran.r-project.org/src/contrib/Archive/ggplot2/ggplot2_0.4.5.tar.gz"
w <- sub('.*\\/', '', package)
w
sub('.tar.*', '', w)
sub('.*\\_', '', c)
# Extract the package name from the URL
a <- "https://cran.r-project.org/src/contrib/Archive/ggplot2/ggplot2_0.4.5.tar.gz"
b <- sub('.*\\/', '', a)
c <- sub('.tar.*', '', b)
sub('.*\\_', '', c)
sub('\\._*', '', c)
c
sub('\\_*', '', c)
a <- "https://cran.r-project.org/src/contrib/Archive/ggplot2/ggplot2_0.4.5.tar.gz"
b <- sub('.*\\/', '', a)
c <- sub('.tar.*', '', b)
c
sub('.*\\_', '', c)
bar <- sub('\\_.*', '', c)
sub('\\_.*', '', c)
a <- "https://cran.r-project.org/src/contrib/Archive/ggplot2/ggplot2_0.4.5.tar.gz"
strsplit(basename(a), "_")
a <- "https://cran.r-project.org/src/contrib/Archive/ggplot2/ggplot2_0.4.5.tar.gz"
strsplit(basename(a), "_")
sub(".tar.*", "", strsplit(basename(a), "_")[[1]])
sub(".tar.*", "", strsplit(basename(a), "_")[[1]])
sub(".tar.*", "", strsplit(basename(a), "_")[[1]])[1]
sub(".tar.*", "", strsplit(basename(a), "_")[[1]])[2]
sub(".tar.*", "", strsplit(basename(package), "_")[[1]])[1]
sub(".tar.*", "", strsplit(basename(package), "_")[[1]])[2]
# Function is named pkg_function. Takes one function
# as an input
pkg_function <- function(package, version = NA){
# Check if package contains a URL:
url_test <- ifelse(length(grep("http://", package)) +
length(grep("https://", package)) > 0,
TRUE,
FALSE)
# If package name provided but no version, install
# package if not installed.
if(url_test == FALSE & is.na(version) == TRUE){
if(!package %in% installed.packages()[ ,1]){
install.packages(package, repos = 'http://cran.rstudio.com/')
}
# Load package
library(package)
}
# If a package name is provided and a version number is
# provided, check if the version is the most recent on CRAN.
# If so, install as usual if not already installed.
# Otherwise,install regardless of whether it is installed
# (as a different version).
if(url_test == FALSE & is.na(version) == FALSE){
# Create a URL for downloading/installing the package from CRAN
package_url <- paste0("http://cran.r-project.org/src/contrib/Archive/",
package,"/",package,"_",version,".tar.gz")
# If the package isn't installed, install the specified version:
if(!package %in% installed.packages()[ ,1]){
install.packages(package_url, repos = NULL, type = "source")
}
# Check if the package is installed. If it is installed, check
# if the installed version is the same as the specified version.
# If so, do nothing. Otherwise, try to install the specified
# version from CRAN.
if(package %in% installed.packages()[ ,1]){
if(version != installed.packages()[package,3]){
install.packages(package_url, repos = NULL, type = "source")
}
}
# Load package
library(package)
}
# If URL is provided, check if the specified
# package and version is already installed. If so,
# do nothing. Otherwise, install it.
if(url_test == TRUE & is.na(version) == TRUE){
# Extract the package name and version from the URL
pkg_name <- sub(".tar.*", "", strsplit(basename(package), "_")[[1]])[1]
pkg_version <- sub(".tar.*", "", strsplit(basename(package), "_")[[1]])[2]
# If the package isn't installed, install the specified version:
if(!pkg_name %in% installed.packages()[ ,1]){
install.packages(package_url, repos = NULL, type = "source")
library(pkg_name)
}
# If the package is installed, check if the version matches
# the currently loaded version. Install if not installed.
if(package %in% installed.packages()[ ,1]){
if(pkg_version != installed.packages()[package,3]){
install.packages(package_url, repos = NULL, type = "source")
library(pkg_name)
}
}
}
}
# Wrap a for-loop around pkg_function to apply the function to
# each element of a vector of package names and versions.
loadpkg <- function(packages, versions = rep(NA,length(packages))){
# Function is named pkg_function. Takes one function
# as an input
pkg_function <- function(package, version = NA){
# Check if package contains a URL:
url_test <- ifelse(length(grep("http://", package)) +
length(grep("https://", package)) > 0,
TRUE,
FALSE)
# If package name provided but no version, install
# package if not installed. Load it.
if(url_test == FALSE & is.na(version) == TRUE){
if(!package %in% installed.packages()[ ,1]){
install.packages(package, repos = 'http://cran.rstudio.com/')
library(package)
}
}
# If a package name is provided and a version number is
# provided, check if the version is the most recent on CRAN.
# If so, install as usual if not already installed.
# Otherwise,install regardless of whether it is installed
# (as a different version).
if(url_test == FALSE & is.na(version) == FALSE){
# Create a URL for downloading/installing the package from CRAN
package_url <- paste0("http://cran.r-project.org/src/contrib/Archive/",
package,"/",package,"_",version,".tar.gz")
# If the package isn't installed, install the specified version
# and load it.
if(!package %in% installed.packages()[ ,1]){
install.packages(package_url, repos = NULL, type = "source")
library(package)
}
# Check if the package is installed. If it is installed, check
# if the installed version is the same as the specified version.
# If so, do nothing. Otherwise, try to install the specified
# version from CRAN. Load it.
if(package %in% installed.packages()[ ,1]){
if(version != installed.packages()[package,3]){
install.packages(package_url, repos = NULL, type = "source")
library(package)
}
}
}
# If URL is provided, check if the specified
# package and version is already installed. If so,
# do nothing. Otherwise, install it.
if(url_test == TRUE & is.na(version) == TRUE){
# Give a warning Rtools must be installed for windows
warning("You have provided a URL to the package source code.
If you are using Windows, make sure Rtools is installed
to compile the code!")
# Extract the package name and version from the URL
pkg_name <- sub(".tar.*", "", strsplit(basename(package), "_")[[1]])[1]
pkg_version <- sub(".tar.*", "", strsplit(basename(package), "_")[[1]])[2]
# If the package isn't installed, install the specified version
# and load it.
if(!pkg_name %in% installed.packages()[ ,1]){
install.packages(package_url, repos = NULL, type = "source")
library(pkg_name)
}
# If the package is installed, check if the version matches
# the currently loaded version. Install if not installed.
if(package %in% installed.packages()[ ,1]){
if(pkg_version != installed.packages()[package,3]){
install.packages(package_url, repos = NULL, type = "source")
library(pkg_name)
}
}
}
}
# Iterate through the vector of package names/urls
for(i in 1:length(packages)){
pkg_function(package = packages[i], version = version[i])
}
}
# Wrap a for-loop around pkg_function to apply the function to
# each element of a vector of package names and versions.
loadpkg <- function(packages, versions = rep(NA,length(packages))){
# Function is named pkg_function. Takes one function
# as an input
pkg_function <- function(package, version = NA){
# Check if package contains a URL:
url_test <- ifelse(length(grep("http://", package)) +
length(grep("https://", package)) > 0,
TRUE,
FALSE)
# If package name provided but no version, install
# package if not installed. Load it.
if(url_test == FALSE & is.na(version) == TRUE){
if(!package %in% installed.packages()[ ,1]){
install.packages(package, repos = 'http://cran.rstudio.com/')
library(package)
}
}
# If a package name is provided and a version number is
# provided, check if the version is the most recent on CRAN.
# If so, install as usual if not already installed.
# Otherwise,install regardless of whether it is installed
# (as a different version).
if(url_test == FALSE & is.na(version) == FALSE){
# Create a URL for downloading/installing the package from CRAN
package_url <- paste0("http://cran.r-project.org/src/contrib/Archive/",
package,"/",package,"_",version,".tar.gz")
# If the package isn't installed, install the specified version
# and load it.
if(!package %in% installed.packages()[ ,1]){
install.packages(package_url, repos = NULL, type = "source")
library(package)
}
# Check if the package is installed. If it is installed, check
# if the installed version is the same as the specified version.
# If so, do nothing. Otherwise, try to install the specified
# version from CRAN. Load it.
if(package %in% installed.packages()[ ,1]){
if(version != installed.packages()[package,3]){
install.packages(package_url, repos = NULL, type = "source")
library(package)
}
}
}
# If URL is provided, check if the specified
# package and version is already installed. If so,
# do nothing. Otherwise, install it.
if(url_test == TRUE & is.na(version) == TRUE){
# Give a warning Rtools must be installed for windows
warning("You have provided a URL to the package source code.
If you are using Windows, make sure Rtools is installed
to compile the code!")
# Extract the package name and version from the URL
pkg_name <- sub(".tar.*", "", strsplit(basename(package), "_")[[1]])[1]
pkg_version <- sub(".tar.*", "", strsplit(basename(package), "_")[[1]])[2]
# If the package isn't installed, install the specified version
# and load it.
if(!pkg_name %in% installed.packages()[ ,1]){
install.packages(package_url, repos = NULL, type = "source")
library(pkg_name)
}
# If the package is installed, check if the version matches
# the currently loaded version. Install if not installed.
if(package %in% installed.packages()[ ,1]){
if(pkg_version != installed.packages()[package,3]){
install.packages(package_url, repos = NULL, type = "source")
library(pkg_name)
}
}
}
}
# Iterate through the vector of package names/urls
for(i in 1:length(packages)){
pkg_function(package = packages[i], version = version[i])
}
}
loadpkg(package = "https://cran.r-project.org/src/contrib/Archive/ggplot2/ggplot2_0.9.3.tar.gz",
version = "2.0")
?interplot
View(loadpkg)
rm(list=ls())
install.packages(c("backports", "BH", "broom", "checkmate", "cli", "coda", "colorspace", "cowplot", "curl", "data.table", "devtools", "digest", "dplyr", "evaluate", "fansi", "ggplot2", "ggridges", "git2r", "haven", "htmlTable", "htmlwidgets", "httr", "interactionTest", "jsonlite", "knitr", "lme4", "markdown", "mime", "nloptr", "openssl", "pillar", "pkgconfig", "prediction", "psych", "R6", "Rcpp", "RcppArmadillo", "RcppEigen", "readr", "rlang", "rmarkdown", "rstudioapi", "sjlabelled", "snakecase", "stringi", "tibble", "tidyr", "tidyselect", "tinytex", "vegan", "visNetwork", "xfun", "XML", "xtable"))
install.packages(c("assertthat", "callr", "caTools", "cli", "colorspace", "dplyr", "evaluate", "forcats", "fs", "git2r", "glue", "haven", "highr", "Hmisc", "igraph", "knitr", "lazyeval", "lme4", "lmerTest", "mirt", "openssl", "permute", "pkgbuild", "prediction", "processx", "purrr", "R6", "Rcpp", "RcppArmadillo", "rlang", "rmarkdown", "rstudioapi", "sjlabelled", "stringi", "stringr", "tibble", "tidyr", "tinytex", "vegan", "xfun", "XML"))
combn(x = c("dog","cat","meow"), m = 2)
combn(x = c("waPo","WaTi","None"), m = 2)
?combn
combn(x = c("waPo","WaTi","None"), m = 2, simplify = T)
# installing/loading the package:
if(!require(installr)) {
install.packages("installr");
require(installr)
} #load / install+load installr
updateR()
knitr::opts_chunk$set(echo = TRUE)
install.packages("blogdown")
blogdown::install_hugo()
blogdown::update_hugo()
install.packages("blogdown")
blogdown::hugo_version()
library(blogdown)
install.packages("blogdown")
blogdown::install_hugo(force = TRUE)
install.packages("blogdown")
Sys.which("pdflatex")
Sys.which("pdflatex")
file.exists(Sys.which('texi2dvi'))
.rs.is_tex_installed()
.rs.is_tex_installed()
.rs.is_tex_installed()
rs.istexinstalled()
Sys.which('pdflatex')
Sys.which('xelatex')
Sys.getenv("R_ENVIRON")
`Sys.getenv("R_HOME")
Sys.getenv("R_HOME")
Sys.getenv("R_HOME")
install.packages("installr")
library(installr)
installr::check.for.updates.R()
installr::updateR()
install.packages("dplyr")
install.packages("knitr")
Sys.getenv("R_HOME")
Sys.getenv("PATH")
Sys.getenv("R_HOME")
Sys.getenv("PATH")
Sys.setenv(PATH = paste(Sys.getenv("PATH"), "C:\Users\myname\AppData\Local\Programs\MikTex 2.9\miktex\bin\x64", sep=.Platform$path.sep))
Sys.setenv(PATH = paste(Sys.getenv("PATH"), "C:\Users\brett\AppData\Local\Programs\MikTex 2.9\miktex\bin\x64", sep=.Platform$path.sep))
Sys.getenv("PATH")
setwd("C:/Users/brett/Dropbox/professional/research/non_course_research/conjoint_interactions")
\documentclass[12pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TeX Packages and Commands
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{graphicx} % Enable graphics commands
\usepackage{setspace} % Enable double-spacing with \begin{spacing}{2}
%until \end{spacing}.
\usepackage[utf8]{inputenc} % Enable utf8 characters, i.e., accents
%without coding--just type them in.
\usepackage[english]{babel} % English hyphenation and alphabetization.
%Other languages available.
\usepackage{dcolumn} % For decimal-aligned stargazer/texreg output.
\usepackage[colorlinks=true, urlcolor=blue, citecolor=black,
linkcolor=black]{hyperref} % Include hyperlinks with the \url and
% \href commands.\label{key}
\setlength{\tabcolsep}{1pt}	% Make tables slightly narrower by reducing
%space between columns.
\usepackage{afterpage} % adds \clearpage
\usepackage[margin=1in]{geometry} % 1" margins
\usepackage{placeins} % adds \FloatBarrier commands
\newcommand{\R}{\textsf{R}~} %This creates the command \R to typeset the
%name R correctly.
\usepackage{natbib}			% Enable citation commands \citep{}, \citet{}, etc.
\bibpunct{(}{)}{;}{a}{}{,}		% Formatting for in-text citations
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Set up bibliography (needs .bib file)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Main document (title page, body, references)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\SweaveOpts{concordance=TRUE}
<<label=knitr_setup, include=FALSE>>=
library(knitr)
opts_chunk$set(concordance=TRUE, cache=TRUE, warning=FALSE, message=FALSE, echo=FALSE, results='hide')
@
\title{Decision Rules, Not Data Quality: Complex Preferences, Survey Satisfycing, and Conjoint Experimental Design\thanks{Thanks to Kirk Bansak, Jens Hainmueller, Daniel Hopkins, and Teppei Yamamoto for providing excellent replication files. This paper benefited from the helpful feedback of Sunshine Hillygus, Chris Johnston, and Daniel Stegmueller and the financial support of the Institute for Humane Studies at George Mason University and the Social Science Research Institute at Duke University.}}
\author{Brett J. Gall\\
\href{mailto:brett.gall@duke.edu.edu}{brett.gall@duke.edu}
}
\maketitle
\begin{spacing}{1}
\begin{abstract}
Although studies have shown estimates of preferences measured in conjoint experiments are moderately robust to large quantities of conjoint attributes and choices, previous research has exclusively focused on an estimand that may not be of interest to researchers: the average marginal component effect (AMCE). Drawing on data from others' experiments, I show empirically and conceptually that decision-making heuristics participants adopt in response to high-complexity conjoint experiments can produce ACME estimates similar to those observed in low-complexity conjoint experiments yet significantly attenuate estimates of another quantity often of interest: the average component interactive effect (ACIE). This quantity captures the extent to which preferences over the values of one attribute depend on values of another attribute. Conjoint experiment design choices may substantially increase error in measures of preferences over the joint distributions of attributes even if they mrettodestly affect measures of preferences over the marginal distributions of attributes.
\end{abstract}
\bigskip
\noindent Keywords: Conjoint Experiment, Choice Experiment, Measurement, Heuristics, Average Component Marginal Effects, Average Component Interaction Effect
\end{spacing}
\thispagestyle{empty}
\newpage
\clearpage
\setcounter{page}{1}
\begin{spacing}{2}
<<R: load and install packages and functions, echo=FALSE>>=
# Ideally, make sure all packages listed in
# README are installed prior to running this
source("00_convenient.R")
pkgs <- c("sandwich","dplyr", "survey")
loadpkg(pkgs)
@
Social scientists are increasingly using conjoint experiments to measure the determinants of choice and judgement (Haimueller et al. 2013; Knudsen and Johannesson 2018). In the typical conjoint experiment, researchers randomly assign a set of attributes or attribute values (``features'') to a set of profiles, then ask participants to choose, rate, or rank profiles in accordance to a decision-making criterion. For example, researchers may generate a set of profiles representing descriptions of political candidates varying in whether the descriptions include the ideology of the candidates or not and whether those profiles assigned information about ideology are described as liberal or conservative. Researchers then might ask each participant to choose the candidate the participant most prefers for each set of candidates. Conjoint designs are  widely embraced because the randomization of multiple attributes or attribute values, direct comparisons between alternatives, explicit multidimensionality of choice, and ease of collecting many choices over profiles theoretically can yield more precise, sincere, and externallity valid measures than alternative approaches (Hainemueller et al. 2013; Hainmueller et al. 2015).
Although conjoint designs promise causal identification of several estimands (Leeper et al. forthcoming, ), these effects may be of little interest to researchers. As with any survey experiment, whether the research design enables identification of substantively interesting effects depends upon the degree to which the experimental decision-making context approximates the context of the real-world decision of interest. Perfect congruency is not necessary for a model to be useful for understanding the world, yet typical implementations of conjoint designs feature numerous, rarely-acknowledged discrepancies between the decision-making contexts of the real world and the experimental decision.\footnote{Conjoint experiments typically involve participants choosing between text descriptions of profiles in which attributes are clearly labelled when real-world decisions may not feature such explicit tradeoffs or feature text-based comparisons, the experiments are embedded in online surveys where subjects are recruited through monetary incentives and thus administered to populations potentially other than that of interest, subjects often make choices between options they would never see in the real-world, participants typically make multiple, consecutive choices between different sets of profiles despite the extreme rarity and significant lag between many of the choices conjoint designs intend to model (such as which car one would purchase), and the quantity and content of the attributes provided in the designs potentially differ tremendously from those both under consideration and available to real-world decision-makers.} These factors may undermine the validity and reliability of conjoint estimates. This is concerning because conjoint experiments are only useful to the extent that participants rely on decision rules in experiments that reasonable approximate the decision rules they would use when faced with the faced with the real-world decision of interest.
Researchers in decision science and marketing have long sought to understand the effects of design and implementation choices on the reliability and validity of conjoint designs (McCullough and Best 1979; Lines and Denstadli 2004), yet political scientists have only recently begun to examine the effects of design choices on conjoint estimates. Luckily for researchers, recent validation studies suggest estimates from conjoint experiments are fairly robust to at least two ways in which conjoint experiments may fail to emulate the real world: (1) asking participants to make a large number of choices mirroring choices participants may make only a handful of times throughout their entire life (Bansak et al. 2018) and (2) including many profile attributes when real-world choices may involve consideration of a substantially different number of dimensions of comparison between choices (Bansak et al. forthcoming).
While acknowledging ``results may differ in cases where the conjoint survey covers different subject matter'' and ``survey fatigue may be more pronounced...in a more complex content'' , the authors of these studies conclude ``the breaking point (for poor measurement)...appears to be beyond the range of common practice'' (Bansak et al. 2018, p.118). While individuals are often believed to rush through surveys and adopt ``effort-saving heuristics,'' that might cause estimates to change in response to greater complexity and attentiveness, Bansak et al. (2018) ``estimate the degree of degradation in response quality'' and find modest effects of increasing the number of attributes and choice tasks on data quality.
Unfortunately, researchers have mistakenly interpreted these optimistic validation studies to provide conclusions regarding the general robustness of conjoint estimates to choices over these two elements of experimental design. Yet the authors are clear to state they focus exclusively on the robustness of a single estimand: the average marginal component effect (AMCE).\footnote{While I focus on validity studies conducted by and targeted for political scientists, to my knowledge, decades of reliability and validity studies in marketing have also similarly focused on this estimand.} The ACME captures the average difference in the probability of choosing a profile when a feature takes on one value and the probability of choosing a profile when that same feature takes on another value. For example, the ACME may be of interest to researchers wishing to measure the difference in the probabilities of accepting an asylum applicant when the applicant is from Iraq versus Pakistan (Bansak et al. 2016).
Despite the exclusive focus on the ACME in conjoint experiment validation studies in political science, the ACME is often of little interest to researchers. Instead, other estimands may interest the researcher. One such estimand is the average component interactive effect (ACIE). The ACIE (Hainmueller et al. 2013) quantifies how much the effect of varying one profile feature depends on variation in another profile feature. For example, DeSante (2013) effectivley estimates an ACME when examining whether differences in the willingness to provide government assistance to hard-working versus lazy welfare applicants depend on the applicant's race. Another estimand is the average marginal interaction effect (AMIE) (Egami and Imai 2019).\footnote{Although other estimands - such as the average combination effects and average marginal effects (Egami and Imai 2019) - are potentially of interest, we focus on these because of space constraints and because the cognitive complexity of holding preferences over joint distributions of features (rather than marginal distributions) suggest the effect of conjoint features may differ substantially for these estimands.}
%\footnote{While the ACIE can also capture how the effect of varying one or more features varies with participant characteristics (Hainmueller et al. 2013), I do not focus on this type of ACIE.|
I make three contributions in this article. First, I argue previous work estimating the effect of conjoint design on ACMEs do not measure ``data degradation,'' but instead capture the degree of substitution from a baseline decision rule to an alternative class of decision-rules. Regarding design-induced changes in estimates as changes in decision-rules not only highlights the need for understanding the set of decision rules individuals use in conjoint experiments, but also the fact that design-induced changes in decision rules may not affect ACME estimates (as found in previous studies) yet affecting other estimands. For example, increasing the attribute count and number of choices tasks may have little effect on ACME estimates yet cause substantial changes in ACIE estimates. This provides direction for future conjoint experiment validation studies examining the effects of designs on other estimands and suggests researchers should pay closer attention to whether the number of attributes in the experiment approximate the number of attributes or familiarity with the choice task typically found in the real-world choice or judgement of interest. Second, I draw on data from the same experiments showing the robustness of ACME estimates to increasing attribute and choice task counts to provide empirical evidence of the effect of conjoint design on non-ACME estimands. This can help us understand whether the findings of previous studies apply to a broader set of estimands..\footnote{While Bansak et al. (2018) acknowledge variation in the decision-making context beyond that which they analyze might lead to less optimistic results, I examine whether less optimistic results are obtained for an alternative estimand from the exact same decision-making contexts.}
\section*{Decision Rules and Heuristics}
Models of bounded rationality propose decision-making requires resources. When presented with a menu of options, individuals trade off a greater risk of choosing a less preferred option against incurring the costs (e.g. cognitive effort, time) of increasing the precision and accuracy of estimates of the costs and benefits of options under consideration. Judgement and decision-making fully informed by all available information may prove prohibitively costly for complex decisions requiring numerous effortful, time-consuming computations across vast, multidimensional spaces. In the face of such complexity, decision-making based on easily-accessed heuristics (informational shortcuts) can provide an efficient means for individuals to make reasonably-informed choices without significant expenditure of effort or time. As such, individuals may adopt different decision rules depending upon the perceived complexity of the decision-making context and their budget of decision-making resources (e.g. time, attention) - using heuristics or rules of thumb more often when a decision is complex, the marginal benefit to becoming more informed is sufficiently small, or individuals are bored, busy, or tired.
If individuals increasingly rely on heuristics in lieu of perfect information or switch from one heuristic to another as the complexity of the decision context increases, decisions must be made more quickly, and individuals become more tired or bored, the criteria individuals draw on and the weights of those criteria in judgements and decision-making may change. For example, consider a case where someone chooses which of two welfare applicants should receive government assistance. The applicants randomly vary in their racial identification (black or white) and employment status (employed or unemployed). A complex decision rule using all of the information available to the chooser in the design would consider the \textit{joint} distribution of race and employment status: the effect of employment stauts on the choice could depend on race of the person and vice versa. Meanwhile, a simpler decision rule would rely solely on the marginal distributions of race and employment status: race and employment status are not considered simultaneously and independently influence the choice.
That features of the decision context can change the determinants of decision-making poses a challenge to researchers using survey-based approaches to measuring the determinants of choices typically made in the voting booth, workplace, or supermarket. If the complexity of the decision context varies between the survey and the real world, the survey-based measures of the determinants of choice may fail to provide insight into the causes of the real-world choice. While different decision rules and heuristics can yield identical choices and determinants of choice, this is not gauranteed.\footnote{For example, if an individual only cares about a political candidate's partisan affiliation, decision-making under the rule ``vote for the Republican candidate'' would produce identical choices between candidates compared to if that person fully used all information. However, there is no reason the preferences or heuristics must meet this condition.} It is therefore an empirical question whether common discrepancies between the experimental and real-world decision contexts tend to be substantively important.
\section*{Data Quality vs. Decision Rules}
Previous empirical research in political science (Bansak et al. 2018; Bansak et al. forthcoming) has examined the effects of two ways in which the decision contexts of conjoint experiments may depart from the real-world contexts of interest. First, the quantity of information available to individuals in conjoint experiments - operationalized as the number of attributes describing each profile - may exceed that found in the real world context. Second, experimental participants may make so many choices as to become bored and inattentive relative to their expected level of attentiveness during the real-world decision of interest. For convenience, I refer to increasing the amount of information in a conjoint by increasing the number of attributes as increasing the ``choice complexity'' of the decision task and increasing the number of times individuals are asked to make a decision as increasing the ``choice count.''
While not providing the reasoning for their empirical tests, these previous studies primarily focus on the extent to which increasing the choice complexity and count causes individuals to choose as if they are ignoring the information provided in profile descriptions. This is measured through the degree of attenuation of correlations between profile features and profile choices, as well as the total choice variance explained by profile features. According to the authors, the more closely individuals come to ignoring the information in the choice task, the worse the quality of the data.
Interpreting such patterns as measures of data quality is mistaken. Data quality insinuates something has gone wrong with implementation or measurement during data collection. However, such patterns could emerge with perfect implementation. Whether they are indicative of poor measurement depends on what one wishes to measure. While different decision contexts may encourage adoption of different decision rules, whether the decision context of the study reasonably approximates the context of interest in the real world is a measure of external validity. The measured determinants of choosing a political candidate after making nine previous choices in a conjoint might well accurately and reliably capture the determinants of choice if individuals were really required to vote ten times in short succession in real-world elections.
Conceptualizing such attenuation as the degree to which individuals are adopting observationally differentiable decision rules clarifies one of the less plausible assumptions required to accurately interpret the results of conjoint experiments (constant treatment effects) and highlights a significant shortcoming of previous conjoint validation studies. Evidence individuals are ignoring profile attributes in their decision-making is not an issue of data quality, but rather evidence individuals are relying on a certain class of decision rules observationally equivalent to a ``choose at random'' (RC) decision rule. A lack of evidence of choice complexity and count systematically increasing the adoption of such decision rules does not imply choice complexity and count do not affect decision rule adoption. The
\section*{Beyond Random Choice}
Although the RC rule is a low-effort, fast way to make decisions, it is not the only low-effort, fast rule. In fact, there are technically an infinite number of low-effort, fast rules for decision-making. For example, individuals could pick the odd-numbered option, chose the most preferred option based on the first attribute, or even the option with fewer words. There are therefore a large number of alternative decision rules individuals may plausibly adopt in response to complexity and boredom, many of which could theoretically produce little convergence of choice probabilities while leading to substantial differences compared to decision-making under perfect information, attentiveness, unlimited time, etc.
Revisiting our welfare applicant example, if differences in the willingness to provide government assistance to the employed versus the unemployed does not vary across races and vice versa, the complex decision rule drawing on the joint distribution of race and employment status would lead to identical measures of the determinants of willingness to provide government assistance as the simple decision rule considering each independently. However, one would presumably lead to significantly quicker decisions compared to the other. This higlights the possibility that previous studies on the robustness of conjoint experiments may have failed to find evidence the determinants of choice vary substantially with the complexity or boredom of participants not because individuals are largly relying on the same heuristics and decision-rules, but rather because researchers have been looking at quantities that do not vary across the decision rules individuals adopt.
\section*{Beyond the AMCE}
ADD TEXT
\section*{Empirical Strategy}
I draw on the replication files of the two main conjoint experiment validation studies in political science to estimate the causal effects of increasing the count of choice tasks and attributes on ACIE estimates. By drawing on the same data previous researchers have used to conclude increasing attribute and choice task counts pose modest threats to data quality, the decision-making context and population under consideration are held constant and cannot explain observed differences in the robustness of different estimands.
While I follow the original validation studies in estimating all models with the full dataset, I also estimate the same models on two smaller subpopulations of participants The two validation studies draw on a single dataset in which participants were randomly assigned a choice count of between XXXX and 30 choices and each choice was randomly assigned a choice complexity of between XXXX and 30 attributes. This meant a participant theoretically could have completed 30 choice tasks, where each profile featured 30 attributes. Asking participants to complete such a task would be highly unusual in politial science and outside the scope of many studies. Consequentially, I estimate the effect of the full range of choice complexity for the subpopulation of participants assigned to choice counts within the range typically found in political science and the effect of the full range of choice counts for the subpopulation of participants assigned to choice complexity within the range typically found in political science.
Other than the cases where I use the subpopulations, the sole difference between our empirical analyses is the model of decision-making under consideration. This means while they regress the outcome of whether a profile was chosen or not on indicator variables for all values of each attribute except for a baseline value, I typically regress that same outcome on all possible two-way multiplicative interactions between the attributes and their main effects.
\section*{Data}
\section*{Results: The Effect of Choice Count}
The ACIE of interest is expected difference in the probability of choosing an profile who is high in education versus low education
%Equations should be numbered consecutively, with subnumbering (e.g., Equations 5a and 5b) used as appropriate. Appendix equations should be labelled A1, etc. Do not number equations by section.
%Political Analysis uses the Chicago Manual of Style 16th ed., author-date system. All sources are cited within the text in parentheses by author's last name and date of publication, with page numbers as appropriate: (Smith 2007, 65). Authors should consult the Chicago Manual of Style for Reference list citations. Below are examples of some common reference types.
%ll of the data (including original and archival data) used in a paper or letter must be appropriately cited. Citations to data must include information that will make it easy for readers to find the original data sources, and for those original sources to be consistently identified in the future. Data citations should not appear in the paper's author note, acknowledgements, text, footnotes, tables, figures, or supplementary materials. Rather, data citations must appear in the paper's reference list, and contain the name or title of the dataset, the author or authors, any version information, the date of creation of the version used in the paper, and most importantly a persistent data identifier (for example a DOI).
%Some examples:
% Bullock, Will, Kosuke Imai, Jacob Shapiro, "Replication data for: Statistical analysis of endorsement experiments: Measuring support for militant groups in Pakistan", http://hdl.handle.net.proxy.lib.duke.edu/1902.1/14840 V5 [Version], September 5, 2011.
%
% Monogan, Jamie, "Replication data for: A Case for Registering Studies of Political Outcomes: An Application in the 2010 House Elections", http://hdl.handle.net.proxy.lib.duke.edu/1902.1/16470 V6 [Version], June 3, 2013.
\section*{Results: The Effect of Choice Complexity}
\bibliographystyle{ajps}
\bibliography{merit}
\end{spacing}
\newpage
\FloatBarrier
\newpage
\appendix
\section*{Appendix A}
\end{document}
install.packages("blogdown")
blogdown:::new_post_addin()
\section*{Beyond the AMCE}
blogdown:::new_post_addin()
setwd("C:/Users/brett/Dropbox/professional/website/brettgall.hugo")
blogdown:::new_post_addin()
